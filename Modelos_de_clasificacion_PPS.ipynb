{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9S/gw8AAVIcDTGH4cuBFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Melisa7L/PPS/blob/main/Modelos_de_clasificacion_PPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Librerias"
      ],
      "metadata": {
        "id": "upnahL3wxcuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "ca8VJGLwxevN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "GKWcjFpwxgb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click"
      ],
      "metadata": {
        "id": "QYLxA3msxh19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "1a66KHuZxjqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas spacy scikit-learn matplotlib"
      ],
      "metadata": {
        "id": "32oSRUBixnkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "QctEetGlxpay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "AxWE5y1Sxq7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.6.0"
      ],
      "metadata": {
        "id": "1wqkAl6uxrW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn"
      ],
      "metadata": {
        "id": "ObYb1auvxstG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib"
      ],
      "metadata": {
        "id": "DqUQKCemxuK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas spacy scikit-learn gensim matplotlib"
      ],
      "metadata": {
        "id": "eya2p0voxvYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "py6fkxPoyZne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy"
      ],
      "metadata": {
        "id": "gFXKe6f-yeaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "fAGMd2HlyfsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "Vrf_J0iOxGvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOdiCYd_xCfo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from collections import Counter\n",
        "import joblib\n",
        "\n",
        "# Leer el archivo XLSX que ya se limpió anteriormente\n",
        "df = pd.read_excel('Nuevo.xlsx')\n",
        "\n",
        "# Lista de categorías\n",
        "categorias = df['categoria'].unique()\n",
        "\n",
        "# Crear un diccionario para mapear categorías a números\n",
        "categoria_a_numero = {cat: num for num, cat in enumerate(categorias)}\n",
        "# Asignar etiquetas numéricas a las categorías en el DataFrame\n",
        "df['categoria_num'] = df['categoria'].map(categoria_a_numero)\n",
        "\n",
        "# Diccionario para almacenar las palabras más frecuentes por categoría\n",
        "palabras_por_categoria = {}\n",
        "\n",
        "# Aumentar el límite de max_length para spaCy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "nlp.max_length = 4000000  # Aumenta el límite a 4 millones de caracteres\n",
        "\n",
        "for categoria in categorias:\n",
        "    # Filtrar las filas que pertenecen a la categoría específica\n",
        "    df_categoria = df[df['categoria'] == categoria]\n",
        "\n",
        "    # Analizar la columna de texto_noticia para la categoría específica\n",
        "    text = ' '.join(df_categoria['texto_noticia'].astype(str))\n",
        "\n",
        "    # Normalización del texto\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Uso de spaCy para lematización y eliminación de stopwords\n",
        "    doc = nlp(text)\n",
        "    filtered_words = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.pos_ in ('NOUN', 'ADJ', 'VERB') and token.lemma_.lower() not in nlp.Defaults.stop_words\n",
        "        and not token.is_digit  # Excluir números\n",
        "    ]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    # Contar la frecuencia de las palabras utilizando un diccionario\n",
        "    word_freq = Counter(filtered_words)\n",
        "\n",
        "    # Guardar las palabras más frecuentes en un archivo de texto con codificación UTF-8\n",
        "    with open(f'palabras_mas_frecuentes_{categoria}.txt', 'w', encoding='utf-8') as file:\n",
        "        for word, freq in word_freq.most_common():\n",
        "            file.write(f\"{word}: {freq}\\n\")\n",
        "\n",
        "    # Guardar las palabras más frecuentes en el diccionario\n",
        "    palabras_por_categoria[categoria] = [word for word, freq in word_freq.most_common()]\n",
        "\n",
        "# Cargar las palabras más frecuentes desde el archivo de texto\n",
        "palabras_mas_frecuentes = list(word_freq.keys())\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizador_tfidf = TfidfVectorizer(vocabulary=palabras_mas_frecuentes)\n",
        "X_tfidf = vectorizador_tfidf.fit_transform(df['texto_noticia'])\n",
        "\n",
        "# Reducción de dimensionalidad con TruncatedSVD\n",
        "n_components = 100  # Ajusta el número de componentes según tus necesidades\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "X_tfidf_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba de manera estratificada\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_svd, df['categoria_num'], test_size=0.2, random_state=42, stratify=df['categoria_num'])\n",
        "\n",
        "# Inicializar y entrenar el clasificador SVM con un kernel polinómico\n",
        "svm_classifier = SVC(kernel='poly', degree=2, C=1.0)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Almacenar predicciones y etiquetas reales\n",
        "resultados = {\n",
        "    'y_true': y_test.tolist(),\n",
        "    'y_pred': y_pred.tolist(),\n",
        "    'palabras_clasificadas': palabras_por_categoria\n",
        "}\n",
        "\n",
        "# Agregar las predicciones al DataFrame original\n",
        "df['predicted_category'] = svm_classifier.predict(X_tfidf_svd)\n",
        "\n",
        "# Crear un diccionario inverso para mapear números de categoría a nombres de categoría\n",
        "numero_a_categoria = {num: cat for cat, num in categoria_a_numero.items()}\n",
        "\n",
        "# Mapear los números de categoría predichos de vuelta a los nombres de categoría\n",
        "df['predicted_category'] = df['predicted_category'].map(numero_a_categoria)\n",
        "\n",
        "# Guardar el DataFrame con las predicciones en un archivo Excel\n",
        "df.to_excel('Nuevo_con_predicciones.xlsx', index=False)\n",
        "print(\"Archivo 'Nuevo_con_predicciones.xlsx' guardado con las predicciones.\")\n",
        "\n",
        "# Guardar el modelo SVM en un archivo .pkl\n",
        "modelo_svm_filename = 'modelo_svm4.pkl'\n",
        "joblib.dump(svm_classifier, modelo_svm_filename)\n",
        "print(f\"Modelo SVM guardado en {modelo_svm_filename}\")\n",
        "\n",
        "# Guardar el vectorizador TF-IDF en un archivo .pkl\n",
        "vectorizador_tfidf_filename = 'vectorizador_tfidf4.pkl'\n",
        "joblib.dump(vectorizador_tfidf, vectorizador_tfidf_filename)\n",
        "print(f\"Vectorizador TF-IDF guardado en {vectorizador_tfidf_filename}\")\n",
        "\n",
        "# Guardar el modelo TruncatedSVD en un archivo .pkl\n",
        "svd_filename = 'svd3.pkl'\n",
        "joblib.dump(svd, svd_filename)\n",
        "print(f\"Modelo TruncatedSVD guardado en {svd_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Redes Neuronales**"
      ],
      "metadata": {
        "id": "TwvULnCvxyaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import joblib\n",
        "from collections import Counter\n",
        "\n",
        "# Leer el archivo XLSX que ya se limpió anteriormente\n",
        "df = pd.read_excel('Nuevo.xlsx')\n",
        "\n",
        "# Lista de categorías\n",
        "categorias = df['categoria'].unique()\n",
        "\n",
        "# Crear un diccionario para mapear categorías a números\n",
        "le = LabelEncoder()\n",
        "df['categoria_num'] = le.fit_transform(df['categoria'])\n",
        "\n",
        "# Aumentar el límite de max_length para spaCy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "nlp.max_length = 4000000  # Aumenta el límite a 4 millones de caracteres\n",
        "\n",
        "# Tokenización y vectorización del texto\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['texto_noticia'])\n",
        "sequences = tokenizer.texts_to_sequences(df['texto_noticia'])\n",
        "X = pad_sequences(sequences)\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba de manera estratificada\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, to_categorical(df['categoria_num']), test_size=0.2, random_state=42, stratify=to_categorical(df['categoria_num']))\n",
        "\n",
        "# Crear el modelo de red neuronal\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(categorias), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Almacenar el modelo de red neuronal en un archivo .h5\n",
        "modelo_nn_filename = 'modelo_nn.h5'\n",
        "model.save(modelo_nn_filename)\n",
        "print(f\"Modelo de red neuronal guardado en {modelo_nn_filename}\")\n",
        "\n",
        "# Guardar el LabelEncoder en un archivo .pkl\n",
        "label_encoder_filename = 'label_encoder.pkl'\n",
        "joblib.dump(le, label_encoder_filename)\n",
        "print(f\"LabelEncoder guardado en {label_encoder_filename}\")\n",
        "\n",
        "def cargar_modelo_y_label_encoder(modelo_archivo, label_encoder_archivo):\n",
        "    # Cargar el modelo de red neuronal\n",
        "    model = load_model(modelo_archivo)\n",
        "\n",
        "    # Cargar el LabelEncoder\n",
        "    label_encoder = joblib.load(label_encoder_archivo)\n",
        "\n",
        "    return model, label_encoder\n",
        "\n",
        "# Predecir las categorías para los datos de entrenamiento\n",
        "y_train_pred = model.predict_classes(X_train)\n",
        "df_train_pred = df.loc[X_train.index].copy()\n",
        "df_train_pred['categoria_predicha'] = le.inverse_transform(y_train_pred)\n",
        "\n",
        "# Predecir las categorías para los datos de prueba\n",
        "y_test_pred = model.predict_classes(X_test)\n",
        "df_test_pred = df.loc[X_test.index].copy()\n",
        "df_test_pred['categoria_predicha'] = le.inverse_transform(y_test_pred)\n",
        "\n",
        "# Combinar los resultados de entrenamiento y prueba\n",
        "df_pred = pd.concat([df_train_pred, df_test_pred])\n",
        "\n",
        "# Guardar el DataFrame con las predicciones en un archivo Excel\n",
        "df_pred.to_excel('Nuevo_con_predicciones.xlsx', index=False)\n",
        "print(\"Archivo 'Nuevo_con_predicciones.xlsx' guardado con las predicciones.\")\n",
        "\n",
        "def predecir_categoria(modelo, label_encoder, texto):\n",
        "    # Tokenizar y vectorizar el texto\n",
        "    sequence = tokenizer.texts_to_sequences([texto])\n",
        "    X = pad_sequences(sequence, maxlen=X.shape[1])\n",
        "\n",
        "    # Realizar la predicción\n",
        "    prediction = modelo.predict(X)\n",
        "\n",
        "    # Obtener la categoría predicha\n",
        "    predicted_category = label_encoder.inverse_transform([prediction.argmax()])[0]\n",
        "\n",
        "    return predicted_category\n"
      ],
      "metadata": {
        "id": "wJ2YIe62x4Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "_RJPjfDTx87O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from collections import Counter\n",
        "import joblib\n",
        "\n",
        "# Leer el archivo XLSX que ya se limpió anteriormente\n",
        "df = pd.read_excel('Nuevo.xlsx')\n",
        "\n",
        "# Lista de categorías\n",
        "categorias = df['categoria'].unique()\n",
        "\n",
        "# Crear un diccionario para mapear categorías a números\n",
        "categoria_a_numero = {cat: num for num, cat in enumerate(categorias)}\n",
        "# Asignar etiquetas numéricas a las categorías en el DataFrame\n",
        "df['categoria_num'] = df['categoria'].map(categoria_a_numero)\n",
        "\n",
        "# Diccionario para almacenar las palabras más frecuentes por categoría\n",
        "palabras_por_categoria = {}\n",
        "\n",
        "# Aumentar el límite de max_length para spaCy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "nlp.max_length = 4000000  # Aumenta el límite a 4 millones de caracteres\n",
        "\n",
        "for categoria in categorias:\n",
        "    # Filtrar las filas que pertenecen a la categoría específica\n",
        "    df_categoria = df[df['categoria'] == categoria]\n",
        "\n",
        "    # Analizar la columna de texto_noticia para la categoría específica\n",
        "    text = ' '.join(df_categoria['texto_noticia'].astype(str))\n",
        "\n",
        "    # Normalización del texto\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Uso de spaCy para lematización y eliminación de stopwords\n",
        "    doc = nlp(text)\n",
        "    filtered_words = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.pos_ in ('NOUN', 'ADJ', 'VERB') and token.lemma_.lower() not in nlp.Defaults.stop_words\n",
        "        and not token.is_digit  # Excluir números\n",
        "    ]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    # Contar la frecuencia de las palabras utilizando un diccionario\n",
        "    word_freq = Counter(filtered_words)\n",
        "\n",
        "    # Guardar las palabras más frecuentes en un archivo de texto con codificación UTF-8\n",
        "    with open(f'palabras_mas_frecuentes_{categoria}.txt', 'w', encoding='utf-8') as file:\n",
        "        for word, freq in word_freq.most_common():\n",
        "            file.write(f\"{word}: {freq}\\n\")\n",
        "\n",
        "    # Guardar las palabras más frecuentes en el diccionario\n",
        "    palabras_por_categoria[categoria] = [word for word, freq in word_freq.most_common()]\n",
        "\n",
        "# Cargar las palabras más frecuentes desde el archivo de texto\n",
        "palabras_mas_frecuentes = list(word_freq.keys())\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizador_tfidf = TfidfVectorizer(vocabulary=palabras_mas_frecuentes)\n",
        "X_tfidf = vectorizador_tfidf.fit_transform(df['texto_noticia'])\n",
        "\n",
        "# Reducción de dimensionalidad con TruncatedSVD\n",
        "n_components = 100  # Ajusta el número de componentes según tus necesidades\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "X_tfidf_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba de manera estratificada\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_svd, df['categoria_num'], test_size=0.2, random_state=42, stratify=df['categoria_num'])\n",
        "\n",
        "# Inicializar y entrenar el clasificador Random Forest\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Almacenar predicciones y etiquetas reales\n",
        "resultados = {\n",
        "    'y_true': y_test.tolist(),\n",
        "    'y_pred': y_pred.tolist(),\n",
        "    'palabras_clasificadas': palabras_por_categoria\n",
        "}\n",
        "\n",
        "# Agregar las predicciones al DataFrame original\n",
        "df['predicted_category'] = rf_classifier.predict(X_tfidf_svd)\n",
        "\n",
        "# Crear un diccionario inverso para mapear números de categoría a nombres de categoría\n",
        "numero_a_categoria = {num: cat for cat, num in categoria_a_numero.items()}\n",
        "\n",
        "# Mapear los números de categoría predichos de vuelta a los nombres de categoría\n",
        "df['predicted_category'] = df['predicted_category'].map(numero_a_categoria)\n",
        "\n",
        "# Guardar el DataFrame con las predicciones en un archivo Excel\n",
        "df.to_excel('Nuevo_con_predicciones.xlsx', index=False)\n",
        "print(\"Archivo 'Nuevo_con_predicciones.xlsx' guardado con las predicciones.\")\n",
        "\n",
        "# Guardar el modelo Random Forest en un archivo .pkl\n",
        "modelo_rf_filename = 'modelo_rf.pkl'\n",
        "joblib.dump(rf_classifier, modelo_rf_filename)\n",
        "print(f\"Modelo Random Forest guardado en {modelo_rf_filename}\")\n",
        "\n",
        "# Guardar el vectorizador TF-IDF en un archivo .pkl\n",
        "vectorizador_tfidf_filename = 'vectorizador_tfidf.pkl'\n",
        "joblib.dump(vectorizador_tfidf, vectorizador_tfidf_filename)\n",
        "print(f\"Vectorizador TF-IDF guardado en {vectorizador_tfidf_filename}\")\n",
        "\n",
        "# Guardar el modelo TruncatedSVD en un archivo .pkl\n",
        "svd_filename = 'svd.pkl'\n",
        "joblib.dump(svd, svd_filename)\n",
        "print(f\"Modelo TruncatedSVD guardado en {svd_filename}\")\n"
      ],
      "metadata": {
        "id": "FqYaACIix_DG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}